{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Copernicus-Pretrain (GeoTiff / WebDataset format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract the 100-grid subset\n",
    "!mkdir -p ../data/\n",
    "!wget https://huggingface.co/datasets/wangyi111/Copernicus-Pretrain/resolve/main/example_100_grids/fnames_100_union.json.gz -P ../data/\n",
    "!wget https://huggingface.co/datasets/wangyi111/Copernicus-Pretrain/resolve/main/example_100_grids/example_100_geotiff.zip -P ../data/\n",
    "!unzip ../data/example_100_geotiff.zip -d ../data/example_100_geotiff/\n",
    "!rm ../data/example_100_geotiff.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kornia\n",
      "  Using cached kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting kornia_rs>=0.1.0 (from kornia)\n",
      "  Downloading kornia_rs-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from kornia) (24.2)\n",
      "Requirement already satisfied: torch>=1.9.1 in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from kornia) (2.6.0)\n",
      "Requirement already satisfied: filelock in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (1.13.3)\n",
      "Requirement already satisfied: networkx in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from jinja2->torch>=1.9.1->kornia) (3.0.2)\n",
      "Using cached kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
      "Downloading kornia_rs-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kornia_rs, kornia\n",
      "Successfully installed kornia-0.8.0 kornia_rs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install kornia rasterio gzip # torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid ID: ('0906994_128.50_67.25',)\n",
      "dict_keys(['s1_grd', 's2_toa', 's3_olci', 's5p_co', 's5p_no2', 's5p_o3', 's5p_so2', 'dem'])\n",
      "dict_keys(['s1_grd', 's2_toa', 's3_olci', 's5p_co', 's5p_no2', 's5p_o3', 's5p_so2', 'dem'])\n",
      "### S1 GRD ###\n",
      "Number of s1 local patches: 1    Number of time stamps for first local patch: 3\n",
      "Example for one image: torch.Size([1, 2, 268, 267]) ('0906994_128.50_67.25/1087609_128.50_67.25/20210827',)\n",
      "### S2 TOA ###\n",
      "Number of s2 local patches: 1    Number of time stamps for first local patch: 4\n",
      "Example for one image: torch.Size([1, 13, 268, 267]) ('0906994_128.50_67.25/1087609_128.50_67.25/20200215',)\n",
      "### S3 OLCI ###\n",
      "Number of s3 time stamps: 8\n",
      "Example for one image: torch.Size([1, 21, 67, 174]) ('0906994_128.50_67.25/20210313',)\n",
      "### S5P ###\n",
      "Number of s5p time stamps for CO/NO2/O3/SO2: 7 5 11 4\n",
      "Example for one CO image: torch.Size([1, 1, 26, 66]) ('0906994_128.50_67.25/20210401',)\n",
      "Example for one NO2 image: torch.Size([1, 1, 26, 66]) ('0906994_128.50_67.25/20210501',)\n",
      "Example for one O3 image: torch.Size([1, 1, 26, 66]) ('0906994_128.50_67.25/20210101',)\n",
      "Example for one SO2 image: torch.Size([1, 1, 26, 66]) ('0906994_128.50_67.25/20210601',)\n",
      "### DEM ###\n",
      "One DEM image for the grid: torch.Size([1, 1, 936, 2402]) ('0906994_128.50_67.25',)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from copernicuspretrain_dataset_geotiff import CopernicusPretrain\n",
    "import logging\n",
    "logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n",
    "\n",
    "fnames_path = '../data/example_100_geotiff/fnames_100_union.json.gz'\n",
    "root_dir = '../data/example_100_geotiff/'\n",
    "\n",
    "CopernicusPretrain = CopernicusPretrain(\n",
    "    fnames_path, root_dir, \n",
    "    transform_s1=None, transform_s2=None, transform_s3=None, transform_s5p=None, transform_dem=None\n",
    "    )\n",
    "dataloader = DataLoader(CopernicusPretrain, batch_size=1, shuffle=True, num_workers=2) # batch size can only be 1 because of varying number of images per grid\n",
    "\n",
    "for i, (sample, meta_data) in enumerate(dataloader):\n",
    "    #print(i)\n",
    "    print('Grid ID:', meta_data['dem'][0])\n",
    "    print(sample.keys())\n",
    "    print(meta_data.keys())\n",
    "\n",
    "    \n",
    "    print('### S1 GRD ###')\n",
    "    print('Number of s1 local patches:', len(meta_data['s1_grd']), '  ', 'Number of time stamps for first local patch:', len(meta_data['s1_grd'][0]))\n",
    "    print('Example for one image:', sample['s1_grd'][0][0].shape, meta_data['s1_grd'][0][0])\n",
    "    print('### S2 TOA ###')\n",
    "    print('Number of s2 local patches:', len(meta_data['s2_toa']), '  ', 'Number of time stamps for first local patch:', len(meta_data['s2_toa'][0]))\n",
    "    print('Example for one image:', sample['s2_toa'][0][0].shape, meta_data['s2_toa'][0][0])\n",
    "    print('### S3 OLCI ###')\n",
    "    print('Number of s3 time stamps:', len(meta_data['s3_olci']))\n",
    "    print('Example for one image:', sample['s3_olci'][0].shape, meta_data['s3_olci'][0])\n",
    "    print('### S5P ###')\n",
    "    print('Number of s5p time stamps for CO/NO2/O3/SO2:', len(meta_data['s5p_co']), len(meta_data['s5p_no2']), len(meta_data['s5p_o3']), len(meta_data['s5p_so2']))\n",
    "    print('Example for one CO image:', sample['s5p_co'][0].shape, meta_data['s5p_co'][0])\n",
    "    print('Example for one NO2 image:', sample['s5p_no2'][0].shape, meta_data['s5p_no2'][0])\n",
    "    print('Example for one O3 image:', sample['s5p_o3'][0].shape, meta_data['s5p_o3'][0])\n",
    "    print('Example for one SO2 image:', sample['s5p_so2'][0].shape, meta_data['s5p_so2'][0])\n",
    "    print('### DEM ###')\n",
    "    print('One DEM image for the grid:', sample['dem'].shape, meta_data['dem'][0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WebDataset](https://github.com/webdataset/webdataset) is a data storage format designed for efficient large-scale deep learning workloads. It stores datasets as sharded tar archives, allowing direct streaming without extraction, which significantly reduces disk I/O overhead and improves training speed. It is particularly useful for pretraining foundation models on large datasets.\n",
    "\n",
    "The webdataset library is an implementation of PyTorch IterableDataset, which we will use to build a dataloader. One cool thing for webdataset (and other streaming formats) is that you can stream the data from cloud without downloading the whole dataset. This can be done by parsing urls to the paths of the data shards. The loading speed will depend on many things including the network. In this demo, we simply download the data beforehand and store it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the 100-grid subset\n",
    "!mkdir -p ../data/example_100_webdataset/\n",
    "!wget https://huggingface.co/datasets/wangyi111/Copernicus-Pretrain/resolve/main/example_100_grids/example_100_webdataset/example-{000000..000009}.tar -P ../data/example_100_webdataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdataset\n",
      "  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting braceexpand (from webdataset)\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from webdataset) (2.2.3)\n",
      "Requirement already satisfied: pyyaml in /p/project1/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages (from webdataset) (6.0.2)\n",
      "Downloading webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
      "Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Installing collected packages: braceexpand, webdataset\n",
      "Successfully installed braceexpand-0.1.7 webdataset-0.2.111\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/project/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages/webdataset/compat.py:385: UserWarning: WebDataset(shardshuffle=...) is ignored for resampled datasets\n",
      "  warnings.warn(\n",
      "/p/project/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages/webdataset/compat.py:393: UserWarning: set WebDataset(shardshuffle=...) to a positive integer or 0 or False\n",
      "  warnings.warn(\n",
      "/p/project/hai_ssl4eo/wang_yi/software/miniforge3/envs/copernicusfm/lib/python3.10/site-packages/webdataset/pipeline.py:176: UserWarning: .with_length() only sets the value of __len__ for compatibility with some training environments. It does not change the number of samples in an epoch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'s1_grd': ['0772176_-96.00_44.00/0865831_-95.92_43.96/20221121'], 's2_toa': ['0772176_-96.00_44.00/0865831_-95.92_43.96/20210303'], 's3_olci': ['0772176_-96.00_44.00/20210127'], 's5p_co': ['0772176_-96.00_44.00/20211201'], 's5p_no2': ['0772176_-96.00_44.00/20210901'], 's5p_o3': ['0772176_-96.00_44.00/20210201'], 's5p_so2': ['0772176_-96.00_44.00/20211001'], 'dem': ['0772176_-96.00_44.00']}]\n"
     ]
    }
   ],
   "source": [
    "from copernicuspretrain_dataset_webdataset import CopernicusPretrain\n",
    "\n",
    "shards_path = '../data/example_100_webdataset/example-{000000..000009}.tar'\n",
    "data_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "copernicus_pretrain = CopernicusPretrain(shards_path, batch_size=batch_size, num_workers=2, shuffle=10, shardshuffle=True, resampled=True)\n",
    "dataloader = copernicus_pretrain.get_dataloader()\n",
    "\n",
    "# # Unbatch, shuffle between workers, then rebatch. This may explode memory usage?!\n",
    "# dataloader = dataloader.unbatched().shuffle(100).batched(batch_size)\n",
    "\n",
    "# Since we are using resampling, the dataset is infinite; set an artificial epoch size.\n",
    "dataloader = dataloader.with_epoch(data_size // batch_size)\n",
    "dataloader = dataloader.with_length(data_size // batch_size)\n",
    "\n",
    "\n",
    "for sample in dataloader:\n",
    "    # get one image for each modality\n",
    "    sample_s1, sample_s2, sample_s3, sample_co, sample_no2, sample_o3, sample_so2, sample_dem, meta = sample\n",
    "    print(meta)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copernicusfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
